{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with nltk corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note# Model is already attached in the zip file... directly go to loading the model part of the code to upload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from nltk.corpus import reuters\n",
    "import gensim\n",
    "import re\n",
    "from nltk import tokenize\n",
    "import itertools\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# id's of the documents\n",
    "#reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total number of documents\n",
    "len(reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total list of documents\n",
    "documents = reuters.fileids()\n",
    "# To find list of training documents\n",
    "train_doc = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "# To find list of testing documents\n",
    "test_doc = list(filter(lambda doc: doc.startswith(\"test\"), documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to get the actual text\n",
    "reuters.raw(train_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to get the different categories the documents belongs\n",
    "reuters.categories()\n",
    "# we can also find documents that belong to a particular category\n",
    "reuters.categories('acq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input should be a iist of sentences, with each sentence as list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the input for word2Vec Model\n",
    "# converting all documents in a list of sentences\n",
    "list_allsentences = []\n",
    "for doc_id in range(len(train_doc)):\n",
    "    sentence_indoc = tokenize.sent_tokenize(reuters.raw(train_doc[doc_id])) # spliting each document in sentences\n",
    "    #print(len(sentence_indoc))\n",
    "    for sent in sentence_indoc:\n",
    "        list_ofwords = sent.replace('\\n ','').split()\n",
    "        '''code here if preprocessing is required example'''\n",
    "        # -->\n",
    "        l=[]\n",
    "        for word in list_ofwords:\n",
    "            word = word.lower()\n",
    "            word = re.sub('[,.]','',word)\n",
    "            word = re.sub('[\\d]+','NUM',word) # converting all digits to num\n",
    "            \n",
    "        #    if word not in stopwords:  # removing stopwords\n",
    "            l.append(word)\n",
    "        list_allsentences.append(l)\n",
    "        #list_allsentences.append(list_ofwords)\n",
    "print(len(list_allsentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking the size of the input\n",
    "sys.getsizeof(list_allsentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the models for Wikipedia scraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "data = []\n",
    "path = './Wikipedia_Scrap/'\n",
    "files = [f for f in os.listdir(path) if f.endswith(\".txt\")]\n",
    "for f in files:\n",
    "    with open(os.path.join(path, f)) as myfile:\n",
    "        data.append(myfile.read())\n",
    "wiki_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "wiki_df.columns = ['Articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "tokenize.sent_tokenize(wiki_df['Articles'][0])[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bringing the wiki dataset in input format for word2vec\n",
    "list_wikisentences = []\n",
    "for doc_id in range(len(wiki_df)):\n",
    "    docu =  wiki_df['Articles'][doc_id]\n",
    "    sent_indoc =  tokenize.sent_tokenize(docu)\n",
    "    \n",
    "    for sent in sent_indoc:\n",
    "        list_ofwords = sent.replace('\\n','').split()\n",
    "        \n",
    "        l=[]\n",
    "        for word in list_ofwords:\n",
    "            word=word.lower()\n",
    "            word=re.sub('[,.=?-]','',word)\n",
    "            word = re.sub('[\\d]+','NUM', word)\n",
    "            \n",
    "            l.append(word)\n",
    "        list_wikisentences.append(l)\n",
    "print(len(list_wikisentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking the size of the input\n",
    "sys.getsizeof(list_wikisentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# joining the two set of sentences to create a final training set\n",
    "training_list =list_allsentences + list_wikisentences\n",
    "len(training_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec model with CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =  gensim.models.Word2Vec(training_list, min_count=10, size=100)\n",
    "# size: is the number of topic/ neurons in the training layer\n",
    "# window: tells the number of words to look in the context\n",
    "# alpha: is the initial learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec with Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sg = gensim.models.Word2Vec(training_list, min_count=10, size=100, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "# saving the model\n",
    "model.save('/home/prateek/uva/CS-6501 Text Mining/Workspace/Word2Vec/mymodel1_cbow')\n",
    "model_sg.save('/home/prateek/uva/CS-6501 Text Mining/Workspace/Word2Vec/mymodel1_sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The CBOW model\n",
    "cbow_model = gensim.models.Word2Vec.load('/home/prateek/uva/CS-6501 Text Mining/Workspace/Word2Vec/mymodel1_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Skip gram model\n",
    "sg_model = gensim.models.Word2Vec.load('/home/prateek/uva/CS-6501 Text Mining/Workspace/Word2Vec/mymodel1_sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculating the sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create vector representation of a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs:\n",
    "# words: it is the sentence as a list of tokens\n",
    "# model : skipgram or cbow word2vec model created/loaded above\n",
    "# num_features: Number f neurons in the training layer( which makes the final features)\n",
    "#index2words; list containing names of words in the vocabulary\n",
    "def avg_feature_vector(words, model, num_features, index2word_set):\n",
    "        #function to average all words vectors in a given paragraph\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\") # np.zeros; returns a new array with given \n",
    "                                                                # dimension(Num of neurons in training layers) \n",
    "                                                                # filled with zero\n",
    "        nwords = 0\n",
    "        #index2word_set = set(model.index2word) as set performs better than a list\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                nwords = nwords+1\n",
    "                featureVec = np.add(featureVec, model[word]) # creating the new vector representation\n",
    "\n",
    "        if(nwords>0):\n",
    "            featureVec = np.divide(featureVec, nwords) # normalising the sentences\n",
    "        return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = \"thank you for smoking\".split()\n",
    "w1 = \"the quick brown fox jumped over the lazy dog\".split()\n",
    "w_feat = avg_feature_vector(w, model_sg, 100, set(model.index2word))\n",
    "w1_feat = avg_feature_vector(w1, model_sg, 100, set(model.index2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculating the cosine similarity usin scipy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sen1_sen2_similarity =  1 - scipy.spatial.distance.cosine(w1_feat, w_feat)\n",
    "sen1_sen2_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------- Rough Work ------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_cosine(vec1, vec2):\n",
    "#      intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "#      numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "#      sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "#      sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "#      denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "#      if not denominator:\n",
    "#         return 0.0\n",
    "#      else:\n",
    "#         return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # #get average vector for sentence 1\n",
    "# sentence_1 = \"this is sentence number one\"\n",
    "# sentence_1_avg_vector = avg_feature_vector(sentence_1.split(), model=word2vec_model, num_features=300)\n",
    "\n",
    "# #get average vector for sentence 2\n",
    "# sentence_2 = \"this is sentence number two\"\n",
    "# sentence_2_avg_vector = avg_feature_vector(sentence_2.split(), model=word2vec_model, num_features=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following code was pcked from web,(I don't remember the link)\n",
    "# def collection_stats():\n",
    "#     # List of documents\n",
    "#     documents = reuters.fileids()\n",
    "#     print(str(len(documents)) + \" documents\")\n",
    " \n",
    "#     train_docs = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "#     print(str(len(train_docs)) + \" total train documents\")\n",
    " \n",
    "#     test_docs = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "#     print(str(len(test_docs)) + \" total test documents\")\n",
    " \n",
    "#     # List of categories\n",
    "#     categories = reuters.categories()\n",
    "#     print(str(len(categories)) + \" categories\")\n",
    " \n",
    "#     # Documents in a category\n",
    "#     category_docs = reuters.fileids(\"acq\")\n",
    " \n",
    "#     # Words for a document\n",
    "#     document_id = category_docs[0]\n",
    "#     document_words = reuters.words(category_docs[0])\n",
    "#     print(document_words);  \n",
    " \n",
    "#     # Raw document\n",
    "#     print(reuters.raw(document_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # breaking paragraph into sentences\n",
    "# # 1st implementation using nltk\n",
    "# print(tokenize.sent_tokenize(reuters.raw(train_doc[0]))) #document is broken into sentences\n",
    "# print(tokenize.sent_tokenize(reuters.raw(train_doc[0]))[0].replace('\\n ','').split()) # sentences are further broken\n",
    "# # into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 2nd implementation, using itertools\n",
    "# # taken from 'http://stackoverflow.com/questions/9474395/how-to-break-up-a-paragraph-by-sentences-in-python'\n",
    "# def get_first_n_sentence(text, n):\n",
    "#     endsentence = \".?!\"\n",
    "#     sentences = itertools.groupby(text, lambda x: any(x.endswith(punct) for punct in endsentence))\n",
    "#     for number,(truth, sentence) in enumerate(sentences):\n",
    "#         if truth:\n",
    "#             first_n_sentences = previous+''.join(sentence).replace('\\n',' ')\n",
    "#         previous = ''.join(sentence)\n",
    "#         if number>=2*n: break #\n",
    "\n",
    "#     return first_n_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# checking\n",
    "# print(get_first_n_sentence(reuters.raw(train_doc[0]), 1).replace('\\n ',''))\n",
    "# print(get_first_n_sentence(reuters.raw(train_doc[0]), 1).replace('\\n ','').split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # checking\n",
    "# # printing a list of sentences for a document\n",
    "# print(len(train_doc)) # total training documents\n",
    "# print(len(tokenize.sent_tokenize(reuters.raw(train_doc[0])))) # number of sentences in one document\n",
    "# for num in range(len(tokenize.sent_tokenize(reuters.raw(train_doc[0])))):\n",
    "#     print(tokenize.sent_tokenize(reuters.raw(train_doc[0]))[num].replace('\\n ','').split()) # each sentence broken into\n",
    "#     # list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Preprocessing\n",
    "# import pandas as pd\n",
    "# # stopwords list, we can use any available stopwords list \n",
    "# swords = pd.read_csv('english.stop.txt', sep='\\n', header=None)\n",
    "# stopwords = set(list(swords[0]))\n",
    "# l = []\n",
    "# for word in tokenize.sent_tokenize(reuters.raw(train_doc[0]))[2].replace('\\n ','').split():\n",
    "#     word = re.sub('[,.]','',word)\n",
    "#     word = re.sub('[\\d]+','NUM',word) # converting all digits to num\n",
    "#     if word not in stopwords:  # removing stopwords\n",
    "#         l.append(word)  \n",
    "# print(tokenize.sent_tokenize(reuters.raw(train_doc[0]))[2].replace('\\n ','').split())\n",
    "# print(l)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(train_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # converting all documents in a list of sentences\n",
    "# list_allsentences = []\n",
    "# for doc_id in range(len(train_doc)):\n",
    "#     sentence_indoc = tokenize.sent_tokenize(reuters.raw(train_doc[doc_id])) # spliting each document in sentences\n",
    "#     #print(len(sentence_indoc))\n",
    "#     for sent in sentence_indoc:\n",
    "#         list_ofwords = sent.replace('\\n ','').split()\n",
    "#         '''code here if preprocessing is required example'''\n",
    "#         # -->\n",
    "#         l=[]\n",
    "#         for word in list_ofwords:\n",
    "#             word = word.lower()\n",
    "#             word = re.sub('[,.]','',word)\n",
    "#             word = re.sub('[\\d]+','NUM',word) # converting all digits to num\n",
    "            \n",
    "#         #    if word not in stopwords:  # removing stopwords\n",
    "#             l.append(word)\n",
    "#         list_allsentences.append(l)\n",
    "#         #list_allsentences.append(list_ofwords)\n",
    "# print(len(list_allsentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.getsizeof(list_allsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model =  gensim.models.Word2Vec(list_allsentences, min_count=2, size=100)\n",
    "# size: is the number of topic/ neurons in the training layer\n",
    "# window: tells the number of words to look in the context\n",
    "# alpha: is the initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.most_similar('week', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to estimate memory requirements\n",
    "# model.estimate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_sg = gensim.models.Word2Vec(list_allsentences, min_count=2, size=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model['week']+model['ended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # saving the model\n",
    "# model.save('/home/prateek/uva/CS-6501 Text Mining/Workspace/Word2Vec/mymodel1')\n",
    "# model_sg.save('/home/prateek/uva/CS-6501 Text Mining/Workspace/Word2Vec/mymodel1_sg')\n",
    "# # loading the model\n",
    "# #new_model = gensim.models.Word2Vec.load('/home/prateek/uva/CS-6501 Text Mining/Workspace/Word2Vec/mymodel1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating different similaity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # to calculate the cosine similarity between 2 terms [-1,1]\n",
    "# model.similarity('week','ended')\n",
    "\n",
    "# # to find the most similar terms, based on cosine similarity\n",
    "# model.most_similar('week')\n",
    "# # also,\n",
    "# model.most_similar(positive=['woman', 'king'], negative=['man']) #eg.\n",
    "\n",
    "# # find n most similar words\n",
    "# model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london']) #eg.\n",
    "# # gives multiple similar words, Find the top-N most similar words, using the multiplicative combination objective \n",
    "\n",
    "# # Compute cosine similarity between two sets of words.\n",
    "# model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']) #eg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # to get the vexctor representation of a word; the vectors are numpy arrays\n",
    "# model['term']\n",
    "# # to access all terms in the vocabulary\n",
    "# vocab = list(model.vocab.keys())\n",
    "# vocab[:10] # first 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # to find odd one out\n",
    "# model.doesnt_match(\"breakfast cereal dinner lunch\".split()) #eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # finding multiword phrases like \"new york\"\n",
    "# bigram_transformer = gensim.models.Phrases(sentences)\n",
    "# model = Word2Vec(bigram_transformer[sentences], size=100, ...) \n",
    "\n",
    "# # add sentences \n",
    "# bigram_transformer.add_vocab(new_sentence_stream)\n",
    "# # or\n",
    "# # for trigrams\n",
    "# trigram = Phrases(bigram[sentence_stream])\n",
    "# sent = [u'the', u'new', u'york', u'times', u'is', u'a', u'newspaper']\n",
    "# print(trigram[bigram[sent]])\n",
    "# [u'the', u'new_york_times', u'is', u'a', u'newspaper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(tokenize.sent_tokenize(reuters.raw(test_doc[0]))[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
